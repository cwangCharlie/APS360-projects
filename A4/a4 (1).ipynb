{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4\n",
    "\n",
    "**Deadline**: February 17, 9pm\n",
    "\n",
    "**Late Penalty**: See Syllabus\n",
    "\n",
    "**TA**: Hojjat Salehinejad\n",
    "\n",
    "In this assignment, you will build and train an autoencoder for imputation of missing data. In the process, you will:\n",
    "\n",
    "1. Clean and process continuous and categorical data for machine learning.\n",
    "2. Understand and implement denoising autoencoders.\n",
    "3. Tuning the hyperparameter setting of an autoencoder.\n",
    "\n",
    "### What to submit\n",
    "\n",
    "Submit a PDF file containing all your code and outputs. Do not submit any other files produced by your code.\n",
    "\n",
    "Completing this assignment using Jupyter Notebook is recommended (though not necessarily for all subsequent assignments). If you are using Jupyter Notebook, you can export a PDF file using the menu option File -> Download As -> PDF via LaTeX (pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.utils.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0\n",
    "\n",
    "We will be using a package called `pandas` for this assignment. \n",
    "Installation instructions for `pandas` is available here: \n",
    "https://pandas.pydata.org/pandas-docs/stable/install.html\n",
    "\n",
    "If you cannot get `pandas` installed, you may port the `pandas` code\n",
    "we provided into `numpy` code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. Data Cleaning [12 pt]\n",
    "\n",
    "The data set we will be using for this assignment is the \n",
    "Adult Data Set provided by UCI Machine Learning Repository [1] available \n",
    "at https://archive.ics.uci.edu/ml/datasets/adult.\n",
    "\n",
    "Download the file `adult.data` from the website.\n",
    "\n",
    "The data set contains census record files of adults, including their\n",
    "age, type of work they do, martial status, etc. We will build a denoising\n",
    "autoencoder on this dataset to impute (or \"fill in\") missing values\n",
    "in the dataset.\n",
    "\n",
    "[1] Dua, D. and Karra Taniskidou, E. (2017). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (a) Loading the Data [1 pt]\n",
    "\n",
    "Use the function `pd.read_csv` to load the `adult.data` into a pandas\n",
    "dataframe called `df`. Make sure that\n",
    "the `adult.data` file is in the same folder as your notebook or python code.\n",
    "Report the number of rows (records) in your data frame.\n",
    "\n",
    "Note that the data file does **not** have an index column. The headers\n",
    "of the file are given to you below. \n",
    "\n",
    "Hint: You will need to read a bit about the pandas documentation \n",
    "to do this problem https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = ['age', 'work', 'fnlwgt', 'edu', 'yredu', 'marriage', 'occupation',\n",
    " 'relationship', 'race', 'sex', 'capgain', 'caploss', 'workhr', 'country']\n",
    "df = pd.read_csv # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b) Continuous Features [1 pt]\n",
    "\n",
    "For each of the columns [\"age\", \"yredu\", \"capgain\", \"caploss\", \"workhr\"], find the minimum, maximum, and average value across the dataset.\n",
    "\n",
    "Like numpy arrays and torch tensors, \n",
    "pandas data frames can be sliced. For example, we can\n",
    "display the first 5 rows of the data frame (5 records) below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can slice based on column names, \n",
    "for example `df[\"race\"]`, `df[\"hr\"]`, or even index multiple columns \n",
    "like below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdf = df[[\"age\", \"yredu\", \"capgain\", \"caploss\", \"workhr\"]]\n",
    "subdf[:3] # show the first 3 records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy works nicely with pandas, like below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(subdf[\"caploss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (c) Normalizing Continuous Features [1 pt]\n",
    "\n",
    "Normalize each of the features [\"age\", \"yredu\", \"capgain\", \"caploss\", \"workhr\"]\n",
    "so that their values are between 0 and 1. Just like numpy arrays, you can modify\n",
    "data frames. For example, the code \n",
    "\n",
    "`df[\"age\"] = df[\"age\"] + 1` \n",
    "\n",
    "would increase everyone's age by 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (d) Categorical Features [1 pt]\n",
    "\n",
    "What percentage of people in our data set is male? Note that the data labels all have an unfortunate space in the beginning, e.g. \" Male\" instead of \"Male\".\n",
    "\n",
    "What percentage of people in our data set is female?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hint: you can do something like this in pandas\n",
    "sum(df[\"sex\"] == \" Male\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (e) Missing Values [1 pt]\n",
    "\n",
    "We will do two things in this part:\n",
    "\n",
    "1. We will restrict ourselves to a subset of the features\n",
    "2. We will remove any records (rows) containing missing values, and store it in a second dataframe.\n",
    "\n",
    "Both of these steps are done for you.\n",
    "\n",
    "Report the number of records with and without missing features of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contcols = [\"age\", \"yredu\", \"capgain\", \"caploss\", \"workhr\"]\n",
    "catcols = [\"work\", \"marriage\", \"occupation\", \"edu\", \"relationship\", \"sex\"]\n",
    "features = contcols + catcols\n",
    "df = df[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = pd.concat([df[c] == \" ?\" for c in catcols], axis=1).any(axis=1)\n",
    "df_with_missing = df[missing]\n",
    "df_not_missing = df[~missing]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (f) One-Hot Encoding [1 pt]\n",
    "\n",
    "What are all the possible values of \"work\" in `df_not_missing`? You may find the Python function `set` useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed in class and in tutorial, we will be using a one-hot encoding \n",
    "to encode each of the categorical variables.\n",
    "\n",
    "We will use the pandas function `get_dummies`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.get_dummies(df_not_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (g) One-Hot Encoding [1 pt]\n",
    "\n",
    "How many columns are in the dataframe `data`?\n",
    "\n",
    "Briefly explain where that number come from. (You don't need to be detailed here.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (h) One-Hot Conversion [3 pt]\n",
    "\n",
    "We will convert the pandas data frame into numpy below.\n",
    "However, in doing so, we lose the column information that\n",
    "a panda data frame automatically stores.\n",
    "\n",
    "Complete the function `get_categorical_value` that will return\n",
    "the named value of a feature given a one-hot embedding.\n",
    "You may find the global variables `cat_index` and `cat_values`\n",
    "useful. (Display them and figure out what they are first.)\n",
    "\n",
    "We will use this function on the output of our autoencoder,\n",
    "to interpret our autoencoder outputs. So the input \n",
    "one-hot vectors might not actually be \"one-hot\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datanp = data.values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_index = {}  # Mapping of feature -> start index of feature in a record\n",
    "cat_values = {} # Mapping of feature -> list of categorical values the feature can take\n",
    "\n",
    "# build up the cat_index and cat_values dictionary\n",
    "for i, header in enumerate(data.keys()):\n",
    "    if \"_\" in header: # categorical header\n",
    "        feature, value = header.split()\n",
    "        feature = feature[:-1] # remove the last char; it is always an underscore\n",
    "        if feature not in cat_index:\n",
    "            cat_index[feature] = i\n",
    "            cat_values[feature] = [value]\n",
    "        else:\n",
    "            cat_values[feature].append(value)\n",
    "\n",
    "def get_onehot(record, feature):\n",
    "    \"\"\"\n",
    "    Return the portion of `record` that is the one-hot encoding\n",
    "    of feature. For example, since the feature \"work\" is stored\n",
    "    in the indices [5:12] in each record, calling `get_range(record, \"work\")`\n",
    "    is equivalent to accessing `record[5:12]`.\n",
    "    \n",
    "    Args:\n",
    "        - record: a numpy array representing one record, formatted\n",
    "                  the same way as a row in `data.np`\n",
    "        - feature: a string, should be an element of `catcols`\n",
    "    \"\"\"\n",
    "    start_index = cat_index[feature]\n",
    "    stop_index = cat_index[feature] + len(cat_values[feature])\n",
    "    return record[start_index:stop_index]\n",
    "\n",
    "def get_categorical_value(onehot, feature):\n",
    "    \"\"\"\n",
    "    Return the categorical value name of a feature given\n",
    "    a one-hot vector representing the feature.\n",
    "    \n",
    "    Args:\n",
    "        - onehot: a numpy array one-hot representation of the feature\n",
    "        - feature: a string, should be an element of `catcols`\n",
    "        \n",
    "    Examples:\n",
    "    \n",
    "    >>> get_categorical_value(np.array([0., 0., 0., 0., 0., 1., 0.]), \"work\")\n",
    "    'State-gov'\n",
    "    >>> get_categorical_value(np.array([0.1, 0., 1.1, 0.2, 0., 1., 0.]), \"work\")\n",
    "    'Private'\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "\n",
    "def get_feature(record, feature):\n",
    "    \"\"\"\n",
    "    Return the categorical feature value of a record\n",
    "    \"\"\"\n",
    "    onehot = get_onehot(record, feature)\n",
    "    return get_categorical_value(onehot, feature)\n",
    "\n",
    "def get_features(record):\n",
    "    \"\"\"\n",
    "    Return a dictionary of all categorical feature values of a record\n",
    "    \"\"\"\n",
    "    return { f: get_feature(record, f) for f in catcols }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (i) Train/Test Split [2 pt]\n",
    "\n",
    "Randomly split the data into approximately 70% training, 15% validation and 15% test.\n",
    "\n",
    "Report the number of items in your training, validation, and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(50) # set the numpy seed for consistent split\n",
    "\n",
    "# todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Model Setup [4 pt]\n",
    "\n",
    "Design a fully-connected autoencoder by modifying the `encoder` and `decoder`. \n",
    "\n",
    "There will be a sigmoid activation at the decoder, so that\n",
    "the output of the decoder is between 0 and 1. We will not interpret \n",
    "the output of the sigmoid as a probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(57, 57) # TODO\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(57, 57), # TODO\n",
    "            nn.Sigmoid() # get to the range (0, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. Training [18] \n",
    "\n",
    "### Part (a) [6 pt]\n",
    "\n",
    "We will train our autoencoder as follows:\n",
    "\n",
    "- In each iteration, we will hide one of the categorical features using the `zero_out_random_features` function\n",
    "- We will pass the data with one missing feature through the autoencoder, obtaining a reconstruction\n",
    "- We will check how close the reconstruction is compared to the original data (without the missing feature)\n",
    "\n",
    "Complete the code to train the autoencoder, and plot the training and validation loss every few iterations.\n",
    "You may also want to plot training and validation \"accuracy\" every few iterations, as we will define in\n",
    "part (b). You may also want to checkpoint your model every few epochs.\n",
    "\n",
    "Use `nn.MSELoss()` as your loss function. (Side note: you might recognize that this loss function is not\n",
    "ideal for this problem, but we will use it anyways.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_out_feature(records, feature):\n",
    "    \"\"\" Set the feature missing in records, by setting the appropriate\n",
    "    columns of records to 0\n",
    "    \"\"\"\n",
    "    start_index = cat_index[feature]\n",
    "    stop_index = cat_index[feature] + len(cat_values[feature])\n",
    "    records[:, start_index:stop_index] = 0\n",
    "    return records\n",
    "\n",
    "def zero_out_random_feature(records):\n",
    "    \"\"\" Set one random feature missing in records, by setting the \n",
    "    appropriate columns of records to 0\n",
    "    \"\"\"\n",
    "    return zero_out_feature(records, random.choice(catcols))\n",
    "\n",
    "def train(model, train_loader, valid_loader, num_epochs=5, learning_rate=1e-4):\n",
    "    \"\"\" Training loop. You should update this.\"\"\"\n",
    "    torch.manual_seed(42)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for data in train_loader:\n",
    "            datam = zero_out_random_feature(data.clone()) # zero out one categorical feature\n",
    "            recon = model(datam)\n",
    "            loss = criterion(recon, data)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b) [3 pt]\n",
    "\n",
    "While plotting training and validation loss is valuable, loss values are less easy to compare\n",
    "than accuracy percentages. The reason is that the scale of the loss value changes depending\n",
    "on your batch size. It would be nice to have a measure of \"accuracy\" in this problem.\n",
    "\n",
    "Since we will only be imputing missing categorical values, we will define an accuracy measure.\n",
    "For each record and for each categorical feature, we determine whether\n",
    "the model can predict the categorical feature given all the other features of the record.\n",
    "\n",
    "A function `get_accuracy` is written for you. It is up to you to figure out how to\n",
    "use the function. You don't need to do anything else in this part. To earn the 4 marks,\n",
    "plot the training and validation accuracy every few iterations/epochs as part of your\n",
    "training curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(model, data_loader):\n",
    "    \"\"\"Return the \"accuracy\" of the autoencoder model across a data set\n",
    "    \n",
    "    Args:\n",
    "       - model: the autoencoder model, an instance of nn.Module\n",
    "       - data_loader: an instance of torch.utils.data.DataLoader\n",
    "\n",
    "    Example (to illustrate how get_accuracy is intended to be called.\n",
    "             depending on your variable naming this code might not work\n",
    "             out of the box)\n",
    "\n",
    "        >>> model = AutoEncoder()\n",
    "        >>> vdl = torch.utils.data.DataLoader(data_valid, batch_size=256, shuffle=True)\n",
    "        >>> get_accuracy(model, vdl)\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    acc = 0\n",
    "    for col in catcols:\n",
    "        for item in data_loader: # minibatches\n",
    "            inp = item.detach().numpy()\n",
    "            out = model(zero_out_feature(item.clone(), col)).detach().numpy()\n",
    "            for i in range(out.shape[0]): # record in minibatch\n",
    "                acc += int(get_feature(out[i], col) == get_feature(inp[i], col))\n",
    "                total += 1\n",
    "    return acc / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (c) [4 pt]\n",
    "\n",
    "Run the training code, using reasonable settings like batch_size, learning rate, etc.\n",
    "\n",
    "Include your training curve in your pdf output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (d) [5 pt]\n",
    "\n",
    "Tune your hyperparameters, training at least 4 different models.\n",
    "\n",
    "Do not include all your training curves. Instead, explain what hyperparameters\n",
    "you tried, what their effect was, and what your thought process was as you \n",
    "chose the next set of hyperparameters to try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4. Testing [6 pt]\n",
    "\n",
    "### Part (a) [1 pt]\n",
    "\n",
    "Compute the test accuracy across the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b) [2 pt]\n",
    "\n",
    "Consider an alterative, baseline model that predicts missing data as follows. To predict a missing feature, the\n",
    "baseline model will look at the **most common value** of the feature in the training set. For example, if the feature \"marriage\" is missing, then this model's prediction will be the most common value for \"marriage\" in the training set.\n",
    "\n",
    "What would be the test accuracy of this baseline model?\n",
    "\n",
    "It is often helpful to use the performance of the baseline model to help judge how well our model is actually performing. No explanation is required in this question, just your calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (c) [1 pt]\n",
    "\n",
    "Look at the first item in your test data. \n",
    "Do you think it is reasonable for a human\n",
    "to be able to guess this person's education level\n",
    "based on their other features? Explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (d) [2 pt]\n",
    "\n",
    "What is your model's guess of this person's education\n",
    "level, given their other features?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
